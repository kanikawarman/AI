{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591},{"sourceId":8249738,"sourceType":"datasetVersion","datasetId":4894791},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PDF-based Q&A with Mistral7b, LangChain, and ChromaDB\n\n**Objective:-**\n\nIn this project, we develop an intelligent document analysis system that integrates LLM-based question-answering and PDF file processing capabilities. By leveraging advanced language models such as Mistral7b, we enable users to query and retrieve information directly from PDF documents. The system extracts and analyzes the content of uploaded PDF files, providing accurate and detailed responses to user queries. This project aims to enhance information accessibility and usability by delivering a seamless, efficient tool for interacting with documents.\n\nThis project follows the following **process**:\n\n1. **Document Loading**: Load PDF documents using appropriate functions such as `PDFPlumberLoader`, ensuring efficient handling and processing of the documents.\n\n2. **Text Splitting and Embeddings**: Split the loaded text into manageable chunks for analysis and generate embeddings using an advanced model like Mistral7b. This allows for more precise retrieval and answers from the documents.\n\n3. **Vector Database Integration**: Utilize ChromaDB to create a vector database that stores document embeddings and facilitates efficient retrieval of information. This database serves as the backend for querying and answering user questions.\n\n4. **Create Retrieval Chain**: Develop a retrieval chain that utilizes the ChromaDB vector database to fetch relevant document chunks based on user queries. The retrieval chain integrates the LLM model for answering questions accurately.\n\n5. **Prompt Design and LLM Integration**: Design prompts that incorporate both user queries and document context. Integrate the LLM model to generate refined responses based on the prompts.","metadata":{}},{"cell_type":"markdown","source":"## 1. Install and Import Libraries\n\n            *Make sure to refer to the latest versions and their compatabilities*","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n!pip install -q -U transformers accelerate bitsandbytes langchain tiktoken sentence-transformers chromadb pdfplumber","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T07:22:46.681781Z","iopub.execute_input":"2024-05-01T07:22:46.682453Z","iopub.status.idle":"2024-05-01T07:23:51.340696Z","shell.execute_reply.started":"2024-05-01T07:22:46.682419Z","shell.execute_reply":"2024-05-01T07:23:51.339699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import bfloat16\n# from tempfile import NamedTemporaryFile\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# from transformers import AutoModelForCausalLM\n# from transformers import BitsAndBytesConfig\nimport langchain\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains import RetrievalQA, LLMChain\n# from langchain.chains.combine_documents import create_stuff_documents_chain\n# from langchain_core.prompts import ChatPromptTemplate\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import PDFPlumberLoader, DataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\n# from langchain.vectorstores.base import VectorStore\nfrom langchain.schema.runnable import RunnablePassthrough\nimport chromadb\nfrom chromadb.config import Settings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:23:57.866475Z","iopub.execute_input":"2024-05-01T07:23:57.866879Z","iopub.status.idle":"2024-05-01T07:24:07.332320Z","shell.execute_reply.started":"2024-05-01T07:23:57.866846Z","shell.execute_reply":"2024-05-01T07:24:07.331349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* <b>torch.backends.cuda.enable_mem_efficient_sdp(False)</b>: This line is configuring Torch (PyTorch) to enable or disable memory-efficient structured data parallelism (SDP) for CUDA tensors. SDP is a technique for parallelizing computations across multiple GPUs by dividing data structures across them efficiently. By passing False as an argument, this line indicates that memory-efficient SDP should be disabled.\n\n* <b>torch.backends.cuda.enable_flash_sdp(False)</b>: Similarly, this line is configuring Torch to enable or disable flash structured data parallelism (SDP) for CUDA tensors. Flash SDP is another technique for parallelizing computations across multiple GPUs, potentially offering different trade-offs compared to memory-efficient SDP. By passing False as an argument, this line indicates that flash SDP should be disabled.\n\nI disabled this features, because in my case it's necessary to work with Mistral7b model further. Try and experiment with this feature parameter as False and True.","metadata":{}},{"cell_type":"code","source":"#configure torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:47:55.121514Z","iopub.execute_input":"2024-05-01T07:47:55.122363Z","iopub.status.idle":"2024-05-01T07:47:55.126992Z","shell.execute_reply.started":"2024-05-01T07:47:55.122320Z","shell.execute_reply":"2024-05-01T07:47:55.125987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#execute this if out of memory error occurs\n# import os\n# os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:02.857258Z","iopub.execute_input":"2024-05-01T08:01:02.857668Z","iopub.status.idle":"2024-05-01T08:01:02.862484Z","shell.execute_reply.started":"2024-05-01T08:01:02.857638Z","shell.execute_reply":"2024-05-01T08:01:02.861573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data loading\n\nI have included a pdf file to test the feature. ","metadata":{}},{"cell_type":"code","source":"#import pdf files\nfile_path = '/kaggle/input/aitrends/Data_AI_Trends_Predictions_2024_2.pdf'","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:48:01.207758Z","iopub.execute_input":"2024-05-01T07:48:01.208190Z","iopub.status.idle":"2024-05-01T07:48:01.212462Z","shell.execute_reply.started":"2024-05-01T07:48:01.208160Z","shell.execute_reply":"2024-05-01T07:48:01.211538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the file type (pdf or csv)\nfile_type = \"pdf\" \n\n# Load documents based on file type\ntry:\n    if file_type == \"pdf\":\n        loader = PDFPlumberLoader(file_path)\n    elif file_type == \"csv\":\n        df = pd.read_csv(file_path)\n        loader = DataFrameLoader(df, page_content_column=\"Title\")\n    else:\n        raise ValueError(\"Unsupported file type\")\n    \n    # Load the documents\n    documents = loader.load()\n    print(f\"Loaded {len(documents)} documents from the file.\")\n    \nexcept Exception as e:\n    print(f\"Error loading documents: {e}\")\n    documents = None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:48:04.828144Z","iopub.execute_input":"2024-05-01T07:48:04.828543Z","iopub.status.idle":"2024-05-01T07:48:42.243351Z","shell.execute_reply.started":"2024-05-01T07:48:04.828514Z","shell.execute_reply":"2024-05-01T07:48:42.242428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Verify the loaded content by iterating over the list of documents\nfor i, doc in enumerate(documents):\n    # Print the first 50 characters of each document's content\n    print(f\"Document {i + 1} - Sample: {doc.page_content[:50]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T07:57:15.484982Z","iopub.execute_input":"2024-05-01T07:57:15.485394Z","iopub.status.idle":"2024-05-01T07:57:15.490839Z","shell.execute_reply.started":"2024-05-01T07:57:15.485367Z","shell.execute_reply":"2024-05-01T07:57:15.489945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model Initializing\n\nIn this part, we are initializing the Mistral 7B language model and its corresponding tokenizer. This is crucial because the model and tokenizer are required for processing user queries and generating responses based on the provided PDF documents. \n\nModel Initialization:\n* The AutoModelForCausalLM class from the transformers library is used to load the pre-trained Mistral 7B model from the provided model path (model_path), which points to the model file location on disk. \n* torch_dtype=torch.bfloat16, specifies the data type of the model as bfloat16 for efficient computation\n* device_map=\"auto\", this automatically maps the model to available devices (e.g., CPU or GPU)\n* trust_remote_code=True, this will allow the method to execute code from the model repository that is not part of the official Hugging Face repository. Normally, when you load a pre-trained model, you are just loading the model weights and a standard model architecture from the repository. However, some models may include custom code (e.g., for tokenization, special model layers, or other model-specific processing) that is not part of the official library. This parameter tells the method that you trust the model's repository and allow it to load and execute any custom code that is provided alongside the model.\n\nTokenizer Initialization: \n* The AutoTokenizer class is used to load the tokenizer from the same model_path as the model. The role of tokenizer is to convert input text (user queries and document content) into a format that the model can process. This includes breaking the text into smaller units called tokens and converting them to numerical representations (input IDs) that the model understands.\n* The pad_token of the tokenizer is set to be the same as the eos_token (end-of-sequence token). Tokenizers can add padding to ensure that all input sequences have a uniform length and truncate longer sequences to the maximum length the model can handle. Tokenizers handle special tokens such as start-of-sequence (SOS), end-of-sequence (EOS), and padding tokens that help the model understand the structure of the input.\n* The padding_side is set to \"right\" to pad sequences on the right side, ensuring consistent input length during model inference.","metadata":{}},{"cell_type":"code","source":"model_path=\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n\n#initialize the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype = torch.bfloat16,\n    device_map = \"auto\",\n    trust_remote_code = True\n)\n\n#initialize the tokenizer\ntokenizer=AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:12.371111Z","iopub.execute_input":"2024-05-01T08:01:12.371967Z","iopub.status.idle":"2024-05-01T08:01:14.559032Z","shell.execute_reply.started":"2024-05-01T08:01:12.371934Z","shell.execute_reply":"2024-05-01T08:01:14.558137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Chunking\n\nIt's a process of extracting meaningful phrases, or \"chunks,\" from a sentence based on its grammatical structure and parts of speech. It involves identifying and grouping together contiguous words or tokens that form a syntactic unit, typically consisting of a noun phrase, verb phrase, or prepositional phrase. Later, The split_documents method is called on the text_splitter instance, which processes the input documents and returns the smaller chunks as a list of Document objects.\n\nHere, Chunking can help in text preprocessing, content selection or context segmentation. In this example, we will proceed with Token-based Chunking. However for experimenation purposes, Semantic Chunking was also used to identify the difference between both the methods. Token based chunking yielded better results as our objective is more driven in maintaining semantic meaning and context.\n\nToken-based Chunking:\n* This approach splits text based on tokens (words or subwords), aiming to keep segments within a specified token count. It ensures that each chunk fits within the language model's token limit, which is essential for model compatibility.\n\nSemantic Chunking:\n* This type of chunking uses semantic information to divide the text into coherent and contextually meaningful chunks. For example, the text might be split around paragraphs or sections, depending on the natural breaks in the content.\n\n<b>chunk_size:</b> This parameter specifies the desired size of each chunk in terms of the number of tokens (words or subwords).\n\n<b>chunk_overlap:</b> This parameter specifies the overlap between consecutive chunks. A value of 0 indicates no overlap, meaning each chunk will start exactly where the previous one ends.\n\nThe split_documents method takes your list of documents (e.g., text files or PDFs) and splits them into smaller chunks based on the tokenization and parameters you've specified. The output is a list of chunks that can be used for further processing or retrieval.","metadata":{}},{"cell_type":"code","source":"# Configure text splitter with token based chunking\ntext_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=105)\n\n# Configure text splitter with semantic chunking\n# text_splitter = RecursiveCharacterTextSplitter(\n#     chunk_size=500,  \n#     chunk_overlap=100, \n#     separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"],  # Specify sentence and paragraph separators\n#     keep_separator=True,  # Keep the separator (e.g., sentence end markers) in the chunks\n# )\n\n# Split documents into chunks\ndocs = text_splitter.split_documents(documents)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:22.930130Z","iopub.execute_input":"2024-05-01T08:01:22.930839Z","iopub.status.idle":"2024-05-01T08:01:22.945335Z","shell.execute_reply.started":"2024-05-01T08:01:22.930811Z","shell.execute_reply":"2024-05-01T08:01:22.944406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Storing the data\n\nNow we need to put chunks into an index so that we are able to retrieve them easily when we want to find something in the document or answer questions. We use embedding model and vector database for this purpose.\n\n**Create Embeddings:**\nencoder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n* This line creates an embedding function using the HuggingFaceEmbeddings class from LangChain. The model_name parameter specifies the model to use for generating embeddings. In this case, it uses the \"sentence-transformers/all-MiniLM-L6-v2\" model, which is a pre-trained sentence transformer model known for its efficiency and accuracy in creating sentence embeddings.\n* The encoder converts text documents (or text chunks) into numerical vector representations (embeddings) that capture the semantic meaning of the text. This is necessary for ChromaDB to enable semantic search, allowing the system to match user queries with relevant text chunks based on their meanings rather than exact keyword matches. By using embeddings, ChromaDB can provide more accurate and context-aware search results, leading to better question-answering and information retrieval.\n\n**Create Search Engine/ Database from Provided Documents - ChromaDB:**\n* This code initializes a Chroma vector database using the LangChain library.\n* The docs variable should contain a list of documents (or text chunks) that you want to index in the vector database.\n* The embedding function (encoder) is used to convert the text documents into vector representations.\n* persist_directory: Specifies the directory where the vector database should be persisted (saved). In this case, it uses the .chromadb directory.\n\nBy indexing the documents using the embedding function, you create a searchable database where you can retrieve relevant information based on similarity searches.\n\n**Create Retriever:**\nretriever = search_engine.as_retriever():\n* This line creates a retriever from the search engine (Chroma vector database).\n* The retriever can be used to query the vector database and retrieve relevant documents or information based on the similarity between the query and the indexed documents.\n* Using the retriever, you can integrate the vector database with other components of your application, such as an LLM-based question-answering system.","metadata":{}},{"cell_type":"code","source":"# Create embeddings\nencoder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Create search engine/ database from provided documents - chromadb\nsearch_engine = Chroma.from_documents(\n    documents=docs,\n    embedding=encoder,\n    persist_directory=\".chromadb\"\n)\n\n#intialize the retriever to fetch data from chromaDb based on the query\nretriever = search_engine.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:27.865884Z","iopub.execute_input":"2024-05-01T08:01:27.866787Z","iopub.status.idle":"2024-05-01T08:01:28.868785Z","shell.execute_reply.started":"2024-05-01T08:01:27.866755Z","shell.execute_reply":"2024-05-01T08:01:28.867795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question-answering (QA) system\n\nThe QA system uses a prompt template for generating prompts, a language model chain for generating answers, and a retrieval-augmented generation chain to combine retrieval and generation approaches in the QA process. The below code snippet initializes a text generation pipeline using the Mistral 7b model. This pipeline is responsible for generating responses based on the provided input text. The code sets up a text generation task using the `transformers` library, specifying the model, tokenizer, and parameters for text generation. The `HuggingFacePipeline` instance wraps the text generation pipeline for further integration within the rest of your application.\n\n- **Text Generation Pipeline Initialization**:\n   - **model**: The pre-trained Mistral 7b model instance is provided to the text generation pipeline. This model is responsible for generating the text output based on the provided input.\n    - **tokenizer**: The tokenizer is used to convert input text into tokens that the model can understand. It also helps in converting the model's output tokens back into human-readable text.\n    - **task**: The code specifies the task as \"text-generation,\" which tells the pipeline that it should generate text based on the input provided.\n    - **temperature**: This controls the randomness of the text generation. Lower values make the output more focused and deterministic, while higher values introduce more randomness.\n    - **repetition_penalty**: This parameter discourages repetition in the output text, making the responses more diverse.\n    - **return_full_text**: This specifies whether to return the full text generated or just a part of it.\n    - **max_new_tokens**: This parameter sets the maximum number of new tokens (words) that the model can generate in response to an input prompt.\n\n- **HuggingFacePipeline**:\n    - After setting up the text generation pipeline, the code creates an instance of `HuggingFacePipeline` using the initialized pipeline. This serves as the large language model (LLM) that you can use for further operations, such as responding to user queries.","metadata":{}},{"cell_type":"code","source":"# create a text generation pipeline using Mistral7b\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=300\n)\n\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:33.142835Z","iopub.execute_input":"2024-05-01T08:01:33.143241Z","iopub.status.idle":"2024-05-01T08:01:33.148949Z","shell.execute_reply.started":"2024-05-01T08:01:33.143213Z","shell.execute_reply":"2024-05-01T08:01:33.147793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implement the Pipelines\n\nTo experiment with different approaches and chains, 3 variations have been used to question the database and get responses.\n\n* <b>llm_chain:</b> sets up a generation-based question-answering chain using the LLMChain class. It uses the language model (llm) and the same prompt template. It doesn't use retriever.\n\n* <b>rag_chain:</b> sets up a retrieval-augmented generation (RAG) chain. It combines retrieval (using the retriever) with generation (using the language model).\n\n* <b>RetrievalQA:</b> is a specific type of chain optimized for question-answering tasks by integrating a language model with a retriever to provide accurate and relevant answers to user queries. It is a specific implementation of the RAG approach, focusing on question-answering.","metadata":{}},{"cell_type":"code","source":"# Define common prompt template function\ndef create_prompt_template(context_var=\"context\", question_var=\"question\"):\n    prompt_template = f\"\"\"\n    ### [INST]\n    Instruction: Answer the question based on your data science knowledge. Here is context to help:\n\n    {{{context_var}}}\n\n    ### QUESTION:\n    {{{question_var}}}\n\n    [/INST]\"\"\"\n    return PromptTemplate(input_variables=[context_var, question_var], template=prompt_template)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:01:39.894129Z","iopub.execute_input":"2024-05-01T08:01:39.894505Z","iopub.status.idle":"2024-05-01T08:01:39.899774Z","shell.execute_reply.started":"2024-05-01T08:01:39.894475Z","shell.execute_reply":"2024-05-01T08:01:39.898848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. LLM Chain:\n\nLLM Chain is a straightforward approach that directly uses a language model (LLM) to generate responses to user queries. It should be used when you want to rely purely on the language model's capabilities without incorporating external knowledge.\n\nPros:\n* Simplicity and ease of use.\n* Fast response times as it doesn't require retrieving context from a knowledge base.\n\nCons:\n* May lack accuracy and relevance if the model's training data doesn't include specific or recent information.","metadata":{}},{"cell_type":"code","source":"# Define function to initialize RunnableSequence chain\ndef initialize_llm_chain(llm, context_var=\"context\", question_var=\"question\"):\n    prompt = create_prompt_template(context_var, question_var)\n    # Create a RunnableSequence by chaining the prompt and the LLM using the | operator\n    llm_chain = prompt | llm\n    \n    return llm_chain\n\n# Initialize the chain\nllm_chain = initialize_llm_chain(llm)\n\n# Function to generate answers using LLM chain\ndef generate_answer_with_llm_chain(query):\n    response = llm_chain.invoke({\"context\": \"\", \"question\": query})\n    if '[/INST]' in response:\n        # Split the response at '[/INST]' and return the part after it\n        answer = response.split('[/INST]')[1].strip()\n    else:\n        # If the closing tag is not found, return the whole response as the answer\n        answer = response.strip()\n    \n    return answer\n#     answer = response['text'].split('[/INST]')[1].strip()\n#     return answer","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:51.767944Z","iopub.execute_input":"2024-05-01T08:31:51.768832Z","iopub.status.idle":"2024-05-01T08:31:51.775607Z","shell.execute_reply.started":"2024-05-01T08:31:51.768797Z","shell.execute_reply":"2024-05-01T08:31:51.774682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Give me a summary of data science\"\nprint(\"Answer using LLM Chain:\", generate_answer_with_llm_chain(query))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:31:53.950894Z","iopub.execute_input":"2024-05-01T08:31:53.951285Z","iopub.status.idle":"2024-05-01T08:33:42.207414Z","shell.execute_reply.started":"2024-05-01T08:31:53.951257Z","shell.execute_reply":"2024-05-01T08:33:42.206286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Retrieval-Augmented Generation (RAG):\n\nRAG involves augmenting the language model with external context retrieved from a knowledge base or document database. It could be used when you want to enhance the quality and relevance of responses by providing the model with contextually relevant information.\n\nPros:\n* Produces more accurate and context-aware responses.\n* Can handle a wide variety of tasks beyond question-answering.\n\nCons:\n* May introduce latency due to retrieval of context.\n* Requires careful selection and indexing of the knowledge base.","metadata":{}},{"cell_type":"code","source":"# Define function to initialize RAG chain\ndef initialize_rag_chain(llm, retriever, context_var=\"context\", question_var=\"question\"):\n    prompt = create_prompt_template(context_var, question_var)\n    llm_chain = LLMChain(llm=llm, prompt=prompt)\n    rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain\n    return rag_chain\n\n# Initialize the RAG chain\nrag_chain = initialize_rag_chain(llm, retriever)\n\n# Function to generate answers using RAG chain\ndef generate_answer_with_rag_chain(query):\n    response = rag_chain.invoke(query)\n    answer = response['text'].split('[/INST]')[1].strip()\n    return answer\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:39:55.792684Z","iopub.execute_input":"2024-05-01T08:39:55.793733Z","iopub.status.idle":"2024-05-01T08:39:55.801447Z","shell.execute_reply.started":"2024-05-01T08:39:55.793685Z","shell.execute_reply":"2024-05-01T08:39:55.800486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"What is data science?\"\nprint(\"Answer using RAG Chain:\", generate_answer_with_rag_chain(query))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T08:39:58.781031Z","iopub.execute_input":"2024-05-01T08:39:58.781687Z","iopub.status.idle":"2024-05-01T08:49:33.530245Z","shell.execute_reply.started":"2024-05-01T08:39:58.781657Z","shell.execute_reply":"2024-05-01T08:49:33.529211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. RetrievalQA:\n\nRetrievalQA is a specific type of chain optimized for question-answering tasks using retrieval of relevant context from a document database. It is Used when your primary focus is on answering questions based on context from a document database.\n\nPros:\n* Streamlined for question-answering, providing accurate and informative answers.\n* Optimized integration of language model and retriever.\n\nCons:\n* Less flexible than RAG in terms of tasks it can handle.","metadata":{}},{"cell_type":"code","source":"# # Initialize the chain\n# chain = RetrievalQA.from_chain_type(\n#     llm=llm,  # Mistral 7b model instance\n#     chain_type=\"stuff\",\n#     retriever=search_engine.as_retriever()\n# )","metadata":{"execution":{"iopub.status.busy":"2024-05-01T00:14:10.102700Z","iopub.execute_input":"2024-05-01T00:14:10.103056Z","iopub.status.idle":"2024-05-01T00:14:10.109480Z","shell.execute_reply.started":"2024-05-01T00:14:10.103027Z","shell.execute_reply":"2024-05-01T00:14:10.108502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to initialize the RetrievalQA chain\ndef initialize_retrievalqa_chain(llm, retriever):\n    chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever\n    )\n    return chain\n\n# Function to generate answers using the RetrievalQA chain\ndef generate_answer_with_retrievalqa_chain(query, chain):\n    response = chain.invoke(query)\n    if 'answer' in response:\n        answer = response['answer'].strip()\n    elif 'text' in response:\n        answer = response['text'].strip()\n    else:\n        answer = str(response)\n    \n    return answer\n#     answer = response['text'].strip()\n#     return answer\n\n\n# Initialize the RetrievalQA chain\nretrievalqa_chain = initialize_retrievalqa_chain(llm, search_engine.as_retriever())","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:24:36.643808Z","iopub.execute_input":"2024-05-01T09:24:36.644664Z","iopub.status.idle":"2024-05-01T09:24:36.653030Z","shell.execute_reply.started":"2024-05-01T09:24:36.644629Z","shell.execute_reply":"2024-05-01T09:24:36.651922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate an answer using the RetrievalQA chain\nquery = \"Give me a summary of the document 5\"\nanswer = generate_answer_with_retrievalqa_chain(query, retrievalqa_chain)\nprint(\"Answer using RetrievalQA Chain:\", answer)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:51:43.390429Z","iopub.execute_input":"2024-05-01T09:51:43.390747Z","iopub.status.idle":"2024-05-01T09:59:59.286458Z","shell.execute_reply.started":"2024-05-01T09:51:43.390720Z","shell.execute_reply":"2024-05-01T09:59:59.284460Z"},"trusted":true},"execution_count":null,"outputs":[]}]}