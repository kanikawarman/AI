{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"day-2-classifying-embeddings-with-keras.ipynb","toc_visible":true},"google":{"image_path":"/examples/train_text_classifier_embeddings_files/output_3ae76701e178_0.png","keywords":["examples","googleai","samplecode","python","embed"]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification using Newsgroups Dataset and Embedding-based Model \n\n## Overview\n\nThis project demonstrates the use of text embeddings (produced by the Gemini API) to classify newsgroup posts / articles into various categories. The dataset is preprocessed, embeddings are generated for the text data, and a simple neural network classifier is built to predict the categories.","metadata":{"id":"bhT1u-Pof10V"}},{"cell_type":"markdown","source":"## Description\n\nIn this project, the goal is to classify text data from the 20 Newsgroups dataset using embeddings generated by a pre-trained model. The dataset, which consists of 20 different newsgroup topics, is first preprocessed to clean and extract useful features. Afterward, each text entry is converted into an embedding, which is then used as input to a neural network classifier. The classifier is trained on a subset of categories and evaluated on a test set. The project showcases the process of preparing text data for machine learning, generating embeddings, building a classification model, and predicting new, unseen text entries. Additionally, the project also involves fine-tuning model performance using early stopping and monitoring accuracy.\n\nThis technique uses the Gemini API's embeddings as input, avoiding the need to train on text input directly, and as a result it is able to perform quite well using relatively few examples compared to training a text model from scratch.\n\n### Steps:\n\n1. **Loading the Dataset**:\n   - The 20 Newsgroups dataset is loaded using `fetch_20newsgroups` from `sklearn`, with training and testing subsets.\n\n\n3. **Preprocessing the Text Data**:\n   - Emails are parsed to extract relevant text, including subjects and bodies.\n   - Email addresses are removed, and text is truncated to a length of 5000 characters to standardize the input.\n\n\n4. **Filtering Data by Categories**:\n   - A subset of the dataset is sampled, selecting a specified number of samples per class.\n   - Categories containing the string \"sci\" (for science-related topics) are kept for further processing.\n\n\n5. **Generating Text Embeddings**:\n   - The text data is converted into embeddings using a pre-trained model via a custom `embed_fn` function.\n   - These embeddings serve as vector representations of the text, suitable for input into a machine learning model.\n\n\n6. **Building the Classification Model**:\n   - A simple neural network model is created using Keras with dense layers.\n   - The model is designed to predict the class of the text based on the embeddings.\n\n\n7. **Training the Model**:\n   - The model is trained on the processed text embeddings, with training and validation data split into `x_train`, `y_train`, `x_val`, and `y_val`.\n   - Early stopping is applied to avoid overfitting.\n\n\n8. **Evaluating the Model**:\n   - The trained model is evaluated on the test set to determine its accuracy in predicting the correct categories for new text.\n\n\n9. **Prediction on New Text**:\n   - An example text (not from the training set) is classified by the trained model, and the predicted category probabilities are printed.","metadata":{}},{"cell_type":"markdown","source":"### **Pre-requisite steps**\n\n1. Install the Gemini AI API\n2. Import the necessary libraries\n3. Setup the API key and add that in the key in this notebook\n4. Load the dataset","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Install the API**","metadata":{}},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"id":"FXq0ygI3BCdQ","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:11:49.086139Z","iopub.execute_input":"2024-12-15T10:11:49.086513Z","iopub.status.idle":"2024-12-15T10:12:17.694572Z","shell.execute_reply.started":"2024-12-15T10:11:49.086477Z","shell.execute_reply":"2024-12-15T10:12:17.692990Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Step 2: Install Libraries**","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\n\nimport email\nimport re\n\nimport pandas as pd","metadata":{"id":"XiJjB2vWCQJP","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:17.697167Z","iopub.execute_input":"2024-12-15T10:12:17.697648Z","iopub.status.idle":"2024-12-15T10:12:19.270265Z","shell.execute_reply.started":"2024-12-15T10:12:17.697591Z","shell.execute_reply":"2024-12-15T10:12:19.269160Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Step 3: Set up API key**\n\n* Generate an API key from: [AI Studio](https://aistudio.google.com/app/apikey) using the [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key)\n* Store the API key in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n* To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.\n\n*  *If an error response along the lines of `No user secrets exist for kernel id ...`, is generated, then need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)*","metadata":{"id":"_mwJYXpElYJc"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"id":"tayrk_A2lZ7A","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:19.271888Z","iopub.execute_input":"2024-12-15T10:12:19.273032Z","iopub.status.idle":"2024-12-15T10:12:19.415904Z","shell.execute_reply.started":"2024-12-15T10:12:19.272956Z","shell.execute_reply":"2024-12-15T10:12:19.414565Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Step 4: Load the dataset**\n\nThe [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets. The split between the training and test datasets are based on messages posted before and after a specific date.","metadata":{"id":"C5B9sWq0hNEV"}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names","metadata":{"id":"jDoKis4om-Ea","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:19.419187Z","iopub.execute_input":"2024-12-15T10:12:19.419675Z","iopub.status.idle":"2024-12-15T10:12:30.741152Z","shell.execute_reply.started":"2024-12-15T10:12:19.419621Z","shell.execute_reply":"2024-12-15T10:12:30.740011Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Here is an example of what a record from the training set looks like.","metadata":{"id":"hDz9MjkNl_FD"}},{"cell_type":"code","source":"print(newsgroups_train.data[0])","metadata":{"id":"FPq-56AimOPX","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:30.742466Z","iopub.execute_input":"2024-12-15T10:12:30.742786Z","iopub.status.idle":"2024-12-15T10:12:30.748391Z","shell.execute_reply.started":"2024-12-15T10:12:30.742755Z","shell.execute_reply":"2024-12-15T10:12:30.747376Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def preprocess_newsgroup_row(data):\n    # Extract only the subject and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate each entry to 5,000 characters\n    text = text[:5000]\n\n    return text\n\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"id":"urpLwp3UmPF3","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:30.749701Z","iopub.execute_input":"2024-12-15T10:12:30.750192Z","iopub.status.idle":"2024-12-15T10:12:30.783919Z","shell.execute_reply.started":"2024-12-15T10:12:30.750089Z","shell.execute_reply":"2024-12-15T10:12:30.782302Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Apply preprocessing function to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"id":"JMKddQdNnAOV","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:30.785420Z","iopub.execute_input":"2024-12-15T10:12:30.785819Z","iopub.status.idle":"2024-12-15T10:12:34.502437Z","shell.execute_reply.started":"2024-12-15T10:12:30.785769Z","shell.execute_reply":"2024-12-15T10:12:34.501343Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"To ensure the dataset is balanced and focuses on specific categories, the sample_data function is used. It performs the following operations:\n\n\n* Sampling: Selects a specified number of samples (num_samples) from each class in the dataset.\n* Filtering by Class: The dataset is filtered to only include categories that contain a specific class name, defined by the classes_to_keep parameter.\n* Re-calibrating Label Encoding: After filtering, the label encoding is recalibrated, with each class being assigned a new numeric label.\n","metadata":{"id":"ogEGbg5XDv-T"}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sample rows, selecting num_samples of each Label.\n    df = (\n        df.groupby(\"Label\")[df.columns]\n        .apply(lambda x: x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n\n    # We have fewer categories now, so re-calibrate the label encoding.\n    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n    df[\"Encoded Label\"] = df[\"Class Name\"].cat.codes\n\n    return df","metadata":{"id":"C2N7xXhJohLR","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.504006Z","iopub.execute_input":"2024-12-15T10:12:34.504767Z","iopub.status.idle":"2024-12-15T10:12:34.511233Z","shell.execute_reply.started":"2024-12-15T10:12:34.504715Z","shell.execute_reply":"2024-12-15T10:12:34.509995Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"TRAIN_NUM_SAMPLES = 100\nTEST_NUM_SAMPLES = 25\nCLASSES_TO_KEEP = \"sci\"  # Class name should contain 'sci' to keep science categories\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"id":"jS2g_ZGupBUb","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.512744Z","iopub.execute_input":"2024-12-15T10:12:34.513153Z","iopub.status.idle":"2024-12-15T10:12:34.570550Z","shell.execute_reply.started":"2024-12-15T10:12:34.513106Z","shell.execute_reply":"2024-12-15T10:12:34.569288Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_train.value_counts(\"Class Name\")","metadata":{"id":"j04TMPY8rV5q","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.573795Z","iopub.execute_input":"2024-12-15T10:12:34.574411Z","iopub.status.idle":"2024-12-15T10:12:34.588317Z","shell.execute_reply.started":"2024-12-15T10:12:34.574369Z","shell.execute_reply":"2024-12-15T10:12:34.587052Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Class Name\nsci.crypt          100\nsci.electronics    100\nsci.med            100\nsci.space          100\nName: count, dtype: int64"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df_test.value_counts(\"Class Name\")","metadata":{"id":"qMsnfkVDsJlU","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.589680Z","iopub.execute_input":"2024-12-15T10:12:34.590157Z","iopub.status.idle":"2024-12-15T10:12:34.603762Z","shell.execute_reply.started":"2024-12-15T10:12:34.590107Z","shell.execute_reply":"2024-12-15T10:12:34.602591Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Class Name\nsci.crypt          25\nsci.electronics    25\nsci.med            25\nsci.space          25\nName: count, dtype: int64"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Create the embeddings\n\nTo perform text classification, we need to convert the text data into numerical representations (embeddings) that the model can process. In this step, we use Google’s Generative AI to generate embeddings for each document.\n\nThe following operations are performed:\n\n***Embedding Function (embed_fn):***\n\nThe function sends the text data to Google's Generative AI API, requesting embeddings for classification tasks. The embed_fn function is decorated with a retry mechanism to handle potential API timeouts.\nThe response from the API contains the embedding, which is returned and added to the dataframe.\n\n***Create Embeddings (create_embeddings):***\n\nThis function applies the embed_fn function to each text entry in the dataframe, generating embeddings for the entire dataset.\n\n**NOTE**: Embeddings are computed one at a time, so large sample sizes can take a long time!","metadata":{"id":"Kr-WlKzXjYWn"}},{"cell_type":"markdown","source":"#### Task types\n\nThe `text-embedding-004` model supports a task type parameter that generates embeddings tailored for the specific task.\n\nTask Type | Description\n---       | ---\nRETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\nRETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.\nSEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\nCLASSIFICATION\t| Specifies that the embeddings will be used for classification.\nCLUSTERING\t| Specifies that the embeddings will be used for clustering.\nFACT_VERIFICATION | Specifies that the given text will be used for fact verification.\n\n***We are using Classification embedding type for this task.***","metadata":{"id":"yPECMeE2xYA_"}},{"cell_type":"code","source":"from google.api_core import retry\nfrom tqdm.rich import tqdm\n\n\ntqdm.pandas()\n\n@retry.Retry(timeout=300.0)\ndef embed_fn(text: str) -> list[float]:\n    # You will be performing classification, so set task_type accordingly.\n    response = genai.embed_content(\n        model=\"models/text-embedding-004\", content=text, task_type=\"classification\"\n    )\n\n    return response[\"embedding\"]\n\n\ndef create_embeddings(df):\n    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)\n    return df","metadata":{"id":"MTBGKkPQsotz","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.604796Z","iopub.execute_input":"2024-12-15T10:12:34.605173Z","iopub.status.idle":"2024-12-15T10:12:34.754244Z","shell.execute_reply.started":"2024-12-15T10:12:34.605130Z","shell.execute_reply":"2024-12-15T10:12:34.752872Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df_train = create_embeddings(df_train)\ndf_test = create_embeddings(df_test)","metadata":{"id":"AH0yrHUHtHtw","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:12:34.755442Z","iopub.execute_input":"2024-12-15T10:12:34.755747Z","iopub.status.idle":"2024-12-15T10:14:04.147802Z","shell.execute_reply.started":"2024-12-15T10:12:34.755717Z","shell.execute_reply":"2024-12-15T10:14:04.146508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb24d43e6530406b85afd847a297ddf1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tqdm/std.py:885: TqdmExperimentalWarning: rich is experimental/alpha\n  t = cls(total=total, **tqdm_kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b4301ad743a440a84bbbd091cf57e69"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tqdm/std.py:885: TqdmExperimentalWarning: rich is experimental/alpha\n  t = cls(total=total, **tqdm_kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"6G5TvLlmRjHc","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:04.149408Z","iopub.execute_input":"2024-12-15T10:14:04.149819Z","iopub.status.idle":"2024-12-15T10:14:04.177250Z","shell.execute_reply.started":"2024-12-15T10:14:04.149775Z","shell.execute_reply":"2024-12-15T10:14:04.174314Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                                   Text  Label Class Name  \\\n1100  Stray thought (was Re: More technical details\\...     11  sci.crypt   \n1101  Re: Licensing of public key implementations\\n\\...     11  sci.crypt   \n1102  Re: text of White House announcement and Q&As ...     11  sci.crypt   \n1103  Re: Off the shelf cheap DES keyseach machine (...     11  sci.crypt   \n1104  Re: freely distributable public key cryptograp...     11  sci.crypt   \n\n      Encoded Label                                         Embeddings  \n1100              0  [-0.016891459, 0.026258027, -0.035663858, 0.05...  \n1101              0  [0.023607424, 0.025368966, -0.022163793, 0.027...  \n1102              0  [-0.011158817, 0.019066054, -0.05927952, -0.01...  \n1103              0  [-0.021023186, 0.01573056, -0.025516696, 0.032...  \n1104              0  [0.0020360276, 0.011576817, -0.034181945, 0.02...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Encoded Label</th>\n      <th>Embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1100</th>\n      <td>Stray thought (was Re: More technical details\\...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.016891459, 0.026258027, -0.035663858, 0.05...</td>\n    </tr>\n    <tr>\n      <th>1101</th>\n      <td>Re: Licensing of public key implementations\\n\\...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[0.023607424, 0.025368966, -0.022163793, 0.027...</td>\n    </tr>\n    <tr>\n      <th>1102</th>\n      <td>Re: text of White House announcement and Q&amp;As ...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.011158817, 0.019066054, -0.05927952, -0.01...</td>\n    </tr>\n    <tr>\n      <th>1103</th>\n      <td>Re: Off the shelf cheap DES keyseach machine (...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[-0.021023186, 0.01573056, -0.025516696, 0.032...</td>\n    </tr>\n    <tr>\n      <th>1104</th>\n      <td>Re: freely distributable public key cryptograp...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>0</td>\n      <td>[0.0020360276, 0.011576817, -0.034181945, 0.02...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Build a classification model\n\nIn this step, we construct a neural network model using Keras to perform text classification based on the generated embeddings. The model architecture is designed as follows:\n\n\n* Input Layer: Accepts the embeddings generated for each text entry as input.\n* Hidden Layer: A dense layer with ReLU activation that processes the embeddings.\n* Output Layer: A dense layer with a softmax activation function, producing the probability distribution over the possible classes.\n\nThe build_classification_model function defines this model, and the embedding size is derived from the length of the embeddings (i.e., the number of features in the embeddings).\n\nThe model is then compiled using:\n\n* Loss Function: Sparse categorical cross-entropy (suitable for multi-class classification tasks).\n* Optimizer: Adam optimizer with a learning rate of 0.001.\n* Metrics: Accuracy is tracked during training.\n\nWhile running the model, Keras will take care of details like shuffling the data points, calculating metrics and other ML boilerplate.","metadata":{"id":"QPYEYkIsWt_5"}},{"cell_type":"code","source":"import keras\nfrom keras import layers\n\n\ndef build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n    return keras.Sequential(\n        [\n            layers.Input([input_size], name=\"embedding_inputs\"),\n            layers.Dense(input_size, activation=\"relu\", name=\"hidden\"),\n            layers.Dense(num_classes, activation=\"softmax\", name=\"output_probs\"),\n        ]\n    )","metadata":{"id":"3oLGi4w5JsQR","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:04.179749Z","iopub.execute_input":"2024-12-15T10:14:04.180250Z","iopub.status.idle":"2024-12-15T10:14:17.161300Z","shell.execute_reply.started":"2024-12-15T10:14:04.180201Z","shell.execute_reply":"2024-12-15T10:14:17.160065Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Derive the embedding size from observing the data. The embedding size can also be specified\n# with the `output_dimensionality` parameter to `embed_content` if you need to reduce it.\nembedding_size = len(df_train[\"Embeddings\"].iloc[0])\n\nclassifier = build_classification_model(\n    embedding_size, len(df_train[\"Class Name\"].unique())\n)\nclassifier.summary()\n\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\"accuracy\"],\n)","metadata":{"id":"kORA1Akl5GsG","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:17.162628Z","iopub.execute_input":"2024-12-15T10:14:17.163348Z","iopub.status.idle":"2024-12-15T10:14:17.285401Z","shell.execute_reply.started":"2024-12-15T10:14:17.163310Z","shell.execute_reply":"2024-12-15T10:14:17.284287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ hidden (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m590,592\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ output_probs (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m3,076\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ output_probs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m593,668\u001b[0m (2.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">593,668</span> (2.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Train the model\n\nAfter building and compiling the classification model, the next step is to train it using the preprocessed data. The training process includes the following steps:\n\n***Preparation of Data:***\n\n* The embeddings and corresponding labels for both training and validation datasets are separated into x_train, y_train and x_val, y_val, respectively.\n* The embeddings are stacked into NumPy arrays to serve as input features, while the labels are used as target outputs.\n\n***Early Stopping:***\n\nTo prevent overfitting and save training time, an early stopping callback is used. The training process will terminate if the accuracy metric stabilizes for three consecutive epochs.\n\n***Model Training:***\n\nThe model is trained using the following parameters:\n\n* Epochs: The training runs for a maximum of 20 epochs.\n* Batch Size: 32 samples are processed in each batch.\n* Validation Data: The model evaluates its performance on the validation set during training.\n","metadata":{"id":"kbpTGGiMXDxl"}},{"cell_type":"code","source":"import numpy as np\n\n\nNUM_EPOCHS = 20\nBATCH_SIZE = 32\n\n# Split the x and y components of the train and validation subsets.\ny_train = df_train[\"Encoded Label\"]\nx_train = np.stack(df_train[\"Embeddings\"])\ny_val = df_test[\"Encoded Label\"]\nx_val = np.stack(df_test[\"Embeddings\"])\n\n# Specify that it's OK to stop early if accuracy stabilises.\nearly_stop = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n\n# Train the model for the desired number of epochs.\nhistory = classifier.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    callbacks=[early_stop],\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n)","metadata":{"id":"bGgvMZGfJ1A4","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:17.287224Z","iopub.execute_input":"2024-12-15T10:14:17.287671Z","iopub.status.idle":"2024-12-15T10:14:21.112445Z","shell.execute_reply.started":"2024-12-15T10:14:17.287622Z","shell.execute_reply":"2024-12-15T10:14:21.111282Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.3385 - loss: 1.3636 - val_accuracy: 0.2800 - val_loss: 1.3033\nEpoch 2/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4099 - loss: 1.2407 - val_accuracy: 0.7100 - val_loss: 1.1381\nEpoch 3/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7998 - loss: 1.0561 - val_accuracy: 0.8300 - val_loss: 0.9791\nEpoch 4/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8541 - loss: 0.8948 - val_accuracy: 0.7400 - val_loss: 0.8712\nEpoch 5/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8786 - loss: 0.6886 - val_accuracy: 0.9000 - val_loss: 0.6768\nEpoch 6/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9606 - loss: 0.5400 - val_accuracy: 0.9100 - val_loss: 0.5416\nEpoch 7/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9623 - loss: 0.4190 - val_accuracy: 0.9300 - val_loss: 0.4502\nEpoch 8/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9841 - loss: 0.3069 - val_accuracy: 0.9500 - val_loss: 0.3947\nEpoch 9/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9764 - loss: 0.2357 - val_accuracy: 0.9000 - val_loss: 0.3412\nEpoch 10/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9744 - loss: 0.2124 - val_accuracy: 0.9400 - val_loss: 0.3074\nEpoch 11/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9801 - loss: 0.1668 - val_accuracy: 0.9300 - val_loss: 0.3211\nEpoch 12/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9931 - loss: 0.1654 - val_accuracy: 0.9500 - val_loss: 0.2543\nEpoch 13/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9900 - loss: 0.1245 - val_accuracy: 0.9300 - val_loss: 0.2381\nEpoch 14/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9834 - loss: 0.1020 - val_accuracy: 0.9400 - val_loss: 0.2348\nEpoch 15/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9960 - loss: 0.1035 - val_accuracy: 0.9400 - val_loss: 0.2107\nEpoch 16/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0867 - val_accuracy: 0.9500 - val_loss: 0.1991\nEpoch 17/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9817 - loss: 0.0772 - val_accuracy: 0.9600 - val_loss: 0.1966\nEpoch 18/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9925 - loss: 0.0676 - val_accuracy: 0.9500 - val_loss: 0.1917\nEpoch 19/20\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9959 - loss: 0.0607 - val_accuracy: 0.9500 - val_loss: 0.1760\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Evaluate model performance\n\nAfter training, the model's performance is evaluated on the validation dataset. The evaluation provides insights into how well the model generalizes to unseen data. The following metrics were used:\n\n* **Accuracy**: The percentage of correctly classified samples out of the total samples in the validation set.\n* **Loss**: The categorical cross-entropy loss, which measures the difference between predicted probabilities and true labels.","metadata":{"id":"xGBaDHZUPdJO"}},{"cell_type":"code","source":"classifier.evaluate(x=x_val, y=y_val, return_dict=True)","metadata":{"id":"d2kOeiqqQIB8","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:21.113999Z","iopub.execute_input":"2024-12-15T10:14:21.114649Z","iopub.status.idle":"2024-12-15T10:14:21.211759Z","shell.execute_reply.started":"2024-12-15T10:14:21.114613Z","shell.execute_reply":"2024-12-15T10:14:21.210641Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9477 - loss: 0.1784 \n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.949999988079071, 'loss': 0.17597123980522156}"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"**The high accuracy indicates that the model performs well in classifying the given text data based on the embeddings.**","metadata":{}},{"cell_type":"markdown","source":"## Inference and Prediction\n\nTo test the model's ability to classify new data, we provide an example text that avoids any specific terminology. This approach helps determine if the model generalizes well without bias toward domain-specific jargon.\n\nThe steps are as follows:\n\n**Input Preparation:**\n* A sample text is provided, describing a query about space exploration.\n* The text is embedded using the embed_fn function to create an input vector for the model.\n  \n**Prediction:**\n\n* The embedding is passed to the trained model for classification.\n* The model outputs probabilities for each class, indicating the likelihood that the text belongs to each category.\n  \n**Results:**\n\n* The predicted class probabilities are displayed, showcasing the model's confidence in each category.","metadata":{"id":"XHyP-_torwsm"}},{"cell_type":"code","source":"# This example avoids any space-specific terminology to see if the model avoids\n# biases towards specific jargon.\nnew_text = \"\"\"\nFirst-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access from this 3rd rock?\n\nLet me know how to do that please.\n\"\"\"\nembedded = embed_fn(new_text)","metadata":{"id":"Lj4gR0Mdr2rb","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:21.213024Z","iopub.execute_input":"2024-12-15T10:14:21.213354Z","iopub.status.idle":"2024-12-15T10:14:21.395281Z","shell.execute_reply.started":"2024-12-15T10:14:21.213325Z","shell.execute_reply":"2024-12-15T10:14:21.394020Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Remember that the model takes embeddings as input, and the input must be batched,\n# so here they are passed as a list to provide a batch of 1.\n# inp = np.array([embedded])\n# [result] = classifier.predict(inp)\n\n# for idx, category in enumerate(df_test[\"Class Name\"].cat.categories):\n#     print(f\"{category}: {result[idx] * 100:0.2f}%\")","metadata":{"id":"CKTHEMrRsbcu","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:14:21.396698Z","iopub.execute_input":"2024-12-15T10:14:21.397092Z","iopub.status.idle":"2024-12-15T10:14:21.402231Z","shell.execute_reply.started":"2024-12-15T10:14:21.397054Z","shell.execute_reply":"2024-12-15T10:14:21.400852Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nfrom rich.console import Console\nfrom rich.table import Table\n\n# Predict probabilities\ninp = np.array([embedded])\n[result] = classifier.predict(inp)\n\n# Using Pandas for tabular display\ncategories = df_test[\"Class Name\"].cat.categories\noutput_df = pd.DataFrame({\n    \"Category\": categories,\n    \"Confidence (%)\": [f\"{prob * 100:.2f}%\" for prob in result]\n})\n\n# Display as a Pandas DataFrame\n# print(output_df)\n\n# Using Rich for styled console output\nconsole = Console()\ntable = Table(title=\"Prediction Results\")\n\ntable.add_column(\"Category\", justify=\"left\")\ntable.add_column(\"Confidence (%)\", justify=\"right\")\n\nfor category, prob in zip(categories, result):\n    table.add_row(category, f\"{prob * 100:.2f}%\")\n\nconsole.print(table)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:15:33.483740Z","iopub.execute_input":"2024-12-15T10:15:33.484209Z","iopub.status.idle":"2024-12-15T10:15:33.575576Z","shell.execute_reply.started":"2024-12-15T10:15:33.484172Z","shell.execute_reply":"2024-12-15T10:15:33.574114Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[3m         Prediction Results         \u001b[0m\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mCategory       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConfidence (%)\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ sci.crypt       │          0.06% │\n│ sci.electronics │          0.11% │\n│ sci.med         │          0.01% │\n│ sci.space       │         99.82% │\n└─────────────────┴────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Prediction Results         </span>\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Category        </span>┃<span style=\"font-weight: bold\"> Confidence (%) </span>┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ sci.crypt       │          0.06% │\n│ sci.electronics │          0.11% │\n│ sci.med         │          0.01% │\n│ sci.space       │         99.82% │\n└─────────────────┴────────────────┘\n</pre>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"***Observation**:*\n\n* **The model's prediction demonstrates strong confidence in classifying the input text under the sci.space category.**\n* **Minimal probabilities for other categories, such as sci.crypt, sci.electronics, and sci.med, indicate that the model effectively distinguishes between unrelated topics.**","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis project demonstrates the development of a text classification pipeline, from preprocessing raw text data to embedding generation and training a deep learning model. The model achieves high accuracy and can confidently classify new data, showcasing its generalization capabilities.\n\n### Key Takeaways:\n* Robustness: The model performs well in categorizing data into relevant classes with minimal bias.\n* Versatility: The preprocessing pipeline ensures that the model is adaptable to various textual contexts.\n* Accuracy: A validation accuracy of ~93% highlights the effectiveness of the embedding-based approach.","metadata":{}},{"cell_type":"markdown","source":"### Further Improvements\nWhile the results are promising, there are opportunities for enhancement:\n\n**Data Augmentation:**\n\n* Introduce more diverse text samples to improve generalization across topics.\n* Use techniques like paraphrasing or synonym replacement to increase dataset variety.\n\n**Model Architecture:**\n\n* Experiment with advanced architectures like transformers (e.g., BERT) to handle contextual nuances better.\n* Fine-tune hyperparameters such as learning rate and batch size for improved performance.\n\n**Bias and Fairness:**\n\n* Conduct a thorough bias analysis to ensure the model is not disproportionately favoring certain topics.\n* Include more balanced samples from underrepresented classes.\n\n**Scalability:**\n\n* Optimize the embedding and classification process to handle larger datasets efficiently.\n* Deploy the model using a scalable API for real-time classification.","metadata":{}}]}